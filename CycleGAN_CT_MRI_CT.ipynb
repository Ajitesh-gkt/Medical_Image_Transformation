{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18733,
     "status": "ok",
     "timestamp": 1721031097621,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "VcekVVTFwDUI",
    "outputId": "c10b2917-4b1b-4e17-d09b-7f3d10356261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorflow/examples.git\n",
      "  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-vc0nfcnz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/examples.git /tmp/pip-req-build-vc0nfcnz\n",
      "  Resolved https://github.com/tensorflow/examples.git to commit fff4bcda7201645a1efaea4534403daf5fc03d42\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-examples==0.1703207612.1461250479831370929614362828255168868146460245314) (1.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow-examples==0.1703207612.1461250479831370929614362828255168868146460245314) (1.16.0)\n",
      "Building wheels for collected packages: tensorflow-examples\n",
      "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tensorflow-examples: filename=tensorflow_examples-0.1703207612.1461250479831370929614362828255168868146460245314-py3-none-any.whl size=301584 sha256=37b1f5e171a801d6804b45c8f56a6e36666f5ef13f858d3ddc2b6345fd776c5a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jdb1czoh/wheels/72/5f/d0/7fe769eaa229bf20101d11a357eb23c83c481bee2d7f710599\n",
      "Successfully built tensorflow-examples\n",
      "Installing collected packages: tensorflow-examples\n",
      "Successfully installed tensorflow-examples-0.1703207612.1461250479831370929614362828255168868146460245314\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7133,
     "status": "ok",
     "timestamp": 1721031104752,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "K4K5mdPDwDUV",
    "outputId": "13a697ed-cd12-45a3-b635-0cd9440a6cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "print(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12466,
     "status": "ok",
     "timestamp": 1721031333118,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "QlfxE4Wv7HXS",
    "outputId": "26a30cf6-6212-4eee-dab1-761d3fb44183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "R0cIAUWHwDUY"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "EJ0-towIwDUZ"
   },
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "  # Remove the batch dimension\n",
    "  image = tf.squeeze(image, axis=0)\n",
    "\n",
    "  cropped_image = tf.image.random_crop(\n",
    "      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "  # Add the batch dimension back\n",
    "  cropped_image = tf.expand_dims(cropped_image, axis=0)\n",
    "\n",
    "  return cropped_image\n",
    "\n",
    "# normalizing the images to [-1, 1]\n",
    "def normalize(image):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = (image / 127.5) - 1\n",
    "  return image\n",
    "\n",
    "def random_jitter(image):\n",
    "  # Ensure the image has 3 color channels (RGB)\n",
    "  if image.shape[-1] != 3:\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "\n",
    "  # resizing to 286 x 286 x 3\n",
    "  image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "  # randomly cropping to 256 x 256 x 3\n",
    "  image = random_crop(image)\n",
    "\n",
    "  # random mirroring\n",
    "  image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  return image\n",
    "\n",
    "def preprocess_image_train(image):\n",
    "  image = random_jitter(image)\n",
    "  image = normalize(image)\n",
    "  return image\n",
    "\n",
    "def preprocess_image_test(image):\n",
    "  image = normalize(image)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "nYPx7GnNwDUb"
   },
   "outputs": [],
   "source": [
    "def load_custom_dataset(data_dir, image_size=(IMG_WIDTH, IMG_HEIGHT), batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE):\n",
    "    \"\"\"\n",
    "    Load a custom dataset from specified directories and apply preprocessing steps.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The root directory containing 'trainA', 'trainB', 'testA', and 'testB' folders.\n",
    "        image_size (tuple): The size to resize the images to, e.g., (256, 256).\n",
    "        batch_size (int): The batch size for the dataset.\n",
    "        buffer_size (int): The buffer size for shuffling.\n",
    "\n",
    "    Returns:\n",
    "        train_horses (tf.data.Dataset): Dataset for 'trainA' images.\n",
    "        train_zebras (tf.data.Dataset): Dataset for 'trainB' images.\n",
    "        test_horses (tf.data.Dataset): Dataset for 'testA' images.\n",
    "        test_zebras (tf.data.Dataset): Dataset for 'testB' images.\n",
    "    \"\"\"\n",
    "    # Define data directories\n",
    "    trainA_dir = os.path.join(data_dir, 'train_output/ct')\n",
    "    trainB_dir = os.path.join(data_dir, 'train_output/mri')\n",
    "    testA_dir = os.path.join(data_dir, 'test_output/ct')\n",
    "    testB_dir = os.path.join(data_dir, 'test_output/mri')\n",
    "    sampleA_dir = os.path.join(data_dir,'sample_output/ct')\n",
    "    sampleB_dir = os.path.join(data_dir,'sample_output/mri')\n",
    "\n",
    "    # Define image preprocessing functions (preprocess_image_train and preprocess_image_test)\n",
    "    # Make sure to define these functions according to your specific requirements.\n",
    "    # .shuffle(buffer_size, seed=None, reshuffle_each_iteration=False)   this was the original shuffle function\n",
    "    train_ct = tf.keras.utils.image_dataset_from_directory(\n",
    "        trainA_dir,\n",
    "        label_mode=None,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        # shuffle=True,\n",
    "        # seed = seed\n",
    "    ).cache().map(preprocess_image_train, num_parallel_calls=AUTOTUNE).batch(batch_size)\n",
    "\n",
    "    train_mri = tf.keras.utils.image_dataset_from_directory(\n",
    "        trainB_dir,\n",
    "        label_mode=None,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        # shuffle=True,\n",
    "        # seed = seed\n",
    "    ).cache().map(preprocess_image_train, num_parallel_calls=AUTOTUNE).batch(batch_size)\n",
    "\n",
    "    test_ct = tf.keras.utils.image_dataset_from_directory(\n",
    "        testA_dir,\n",
    "        label_mode=None,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "    ).map(preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().batch(batch_size)\n",
    "\n",
    "    test_mri = tf.keras.utils.image_dataset_from_directory(\n",
    "        testB_dir,\n",
    "        label_mode=None,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "    ).map(preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().batch(batch_size)\n",
    "\n",
    "    sample_ct = tf.keras.utils.image_dataset_from_directory(\n",
    "        sampleA_dir,\n",
    "        label_mode=None,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "    ).map(preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().batch(batch_size)\n",
    "\n",
    "    sample_mri = tf.keras.utils.image_dataset_from_directory(\n",
    "        sampleB_dir,\n",
    "        label_mode=None,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "    ).map(preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().batch(batch_size)\n",
    "\n",
    "    return train_ct, train_mri, test_ct, test_mri, sample_ct, sample_mri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "error",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "HXArZ4LEwDUd",
    "outputId": "cbcbab44-fe1b-4939-f924-18408f18e5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 1 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory /content/drive/MyDrive/dataset/train_output/ct. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8855f8a7726b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_ct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_ct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_custom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2f4086349337>\u001b[0m in \u001b[0;36mload_custom_dataset\u001b[0;34m(data_dir, image_size, batch_size, buffer_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Make sure to define these functions according to your specific requirements.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# .shuffle(buffer_size, seed=None, reshuffle_each_iteration=False)   this was the original shuffle function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     train_ct = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtrainA_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlabel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m         )\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0;34mf\"No images found in directory {directory}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;34mf\"Allowed formats: {ALLOWLIST_FORMATS}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No images found in directory /content/drive/MyDrive/dataset/train_output/ct. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "data_dir='/content/drive/MyDrive/dataset'\n",
    "train_ct, train_mri, test_ct, test_mri, sample_ct, sample_mri=load_custom_dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "lX_INau6wDUf"
   },
   "outputs": [],
   "source": [
    "def remove_batch_dimension(image):\n",
    "    # Remove the first dimension\n",
    "    return image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "By-8GoC2wDUh"
   },
   "outputs": [],
   "source": [
    "train_ct = train_ct.map(remove_batch_dimension)\n",
    "train_mri = train_mri.map(remove_batch_dimension)\n",
    "test_ct = test_ct.map(remove_batch_dimension)\n",
    "test_mri = test_mri.map(remove_batch_dimension)\n",
    "sample_ct = sample_ct.map(remove_batch_dimension)\n",
    "sample_mri = sample_mri.map(remove_batch_dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "3lqR2j4KwDUk"
   },
   "outputs": [],
   "source": [
    "check_ct = next(iter(train_ct))\n",
    "check_mri = next(iter(train_mri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "eg5iQQmTNUry"
   },
   "outputs": [],
   "source": [
    "ground_ct = next(iter(sample_ct))\n",
    "ground_mri = next(iter(sample_mri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "aPP9q_gCmSlb"
   },
   "outputs": [],
   "source": [
    "# plt.imshow(train_ct[0])\n",
    "# plt.show()\n",
    "# plt.imshow(train_mri[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "_XwsmvY4wDUo"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "LcbZGyHFwDUp"
   },
   "outputs": [],
   "source": [
    "to_mri = generator_g(check_ct)\n",
    "to_ct = generator_f(check_mri)\n",
    "plt.figure(figsize=(8, 8))\n",
    "contrast = 8\n",
    "\n",
    "imgs = [check_ct, to_mri, check_mri, to_ct]\n",
    "title = ['CT', 'TO_MRI', 'MRI', 'TO_CT']\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "  plt.subplot(2, 2, i+1)\n",
    "  plt.title(title[i])\n",
    "  if i % 2 == 0:\n",
    "    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n",
    "  else:\n",
    "    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "dwS5wLRtwDUq"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 8))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.title('Is a real MRI?')\n",
    "# plt.imshow(discriminator_y(sample_mri)[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.title('Is a real CT?')\n",
    "# plt.imshow(discriminator_x(sample_ct)[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "-06mXTOwwDUr"
   },
   "outputs": [],
   "source": [
    "LAMBDA = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "hGSHHzfbwDUs"
   },
   "outputs": [],
   "source": [
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333119,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "lwdrEw9rwDUt"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "  real_loss = loss_obj(tf.ones_like(real), real)\n",
    "\n",
    "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "b-COt-dawDUt"
   },
   "outputs": [],
   "source": [
    "def generator_loss(generated):\n",
    "  return loss_obj(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "mR0vYVWjwDUu"
   },
   "outputs": [],
   "source": [
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "  return LAMBDA * loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "V0Kd8nf-wDUu"
   },
   "outputs": [],
   "source": [
    "def identity_loss(real_image, same_image):\n",
    "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "  return LAMBDA * 0.5 * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "khylPGG_wDUu"
   },
   "outputs": [],
   "source": [
    "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "bVQVZdrHwDUv"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/content/drive/MyDrive/dataset/checkpoints\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "PNOz7cfswDUv"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "y8aDZ1qNwDUw"
   },
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "  prediction = model(test_input)\n",
    "\n",
    "  plt.figure(figsize=(12, 12))\n",
    "\n",
    "  display_list = [test_input[0],prediction[0]]\n",
    "  title = ['Input ', 'Predicted ']\n",
    "\n",
    "  for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot it.\n",
    "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "V_zRxrFgwDUw"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "  # persistent is set to True because the tape is used more than\n",
    "  # once to calculate the gradients.\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "    # Generator G translates X -> Y\n",
    "    # Generator F translates Y -> X.\n",
    "\n",
    "    fake_y = generator_g(real_x, training=True)\n",
    "    cycled_x = generator_f(fake_y, training=True)\n",
    "\n",
    "    fake_x = generator_f(real_y, training=True)\n",
    "    cycled_y = generator_g(fake_x, training=True)\n",
    "\n",
    "    # same_x and same_y are used for identity loss.\n",
    "    same_x = generator_f(real_x, training=True)\n",
    "    same_y = generator_g(real_y, training=True)\n",
    "\n",
    "    disc_real_x = discriminator_x(real_x, training=True)\n",
    "    disc_real_y = discriminator_y(real_y, training=True)\n",
    "\n",
    "    disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "    disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "\n",
    "    # calculate the loss\n",
    "    gen_g_loss = generator_loss(disc_fake_y)\n",
    "    gen_f_loss = generator_loss(disc_fake_x)\n",
    "\n",
    "    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
    "\n",
    "    # Total generator loss = adversarial loss + cycle loss\n",
    "    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "\n",
    "    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "\n",
    "  # Calculate the gradients for generator and discriminator\n",
    "  generator_g_gradients = tape.gradient(total_gen_g_loss,\n",
    "                                        generator_g.trainable_variables)\n",
    "  generator_f_gradients = tape.gradient(total_gen_f_loss,\n",
    "                                        generator_f.trainable_variables)\n",
    "\n",
    "  discriminator_x_gradients = tape.gradient(disc_x_loss,\n",
    "                                            discriminator_x.trainable_variables)\n",
    "  discriminator_y_gradients = tape.gradient(disc_y_loss,\n",
    "                                            discriminator_y.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the optimizer\n",
    "  generator_g_optimizer.apply_gradients(zip(generator_g_gradients,\n",
    "                                            generator_g.trainable_variables))\n",
    "\n",
    "  generator_f_optimizer.apply_gradients(zip(generator_f_gradients,\n",
    "                                            generator_f.trainable_variables))\n",
    "\n",
    "  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
    "                                                discriminator_x.trainable_variables))\n",
    "\n",
    "  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
    "                                                discriminator_y.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "rBFf6zTxW2Dq"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# MSE loss function\n",
    "def mse_loss(target, predicted):\n",
    "    return tf.reduce_mean(tf.square(target - predicted))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    n = 0\n",
    "    mse_loss_values = []  # To store MSE loss values for each iteration\n",
    "    cosine_similarity_values = []  # To store Cosine Similarity scores for each iteration\n",
    "    # ssim_loss_values = []  # To store Structural Similarity Index (SSI) Loss values for each iteration\n",
    "\n",
    "    progress_bar = tqdm(tf.data.Dataset.zip((train_ct, train_mri)), desc=f'Epoch {epoch + 1}', total=len(train_ct))\n",
    "\n",
    "    for image_x, image_y in progress_bar:\n",
    "        train_step(image_x, image_y)\n",
    "\n",
    "        # Calculate MSE loss\n",
    "        mse_loss_value = mse_loss(ground_mri, generator_g(image_x))\n",
    "        mse_loss_values.append(mse_loss_value.numpy())\n",
    "\n",
    "        # Calculate Cosine Similarity\n",
    "        cosine_similarity_value = cosine_similarity(ground_mri.numpy().reshape(1, -1), generator_g(image_x).numpy().reshape(1, -1))\n",
    "        cosine_similarity_values.append(cosine_similarity_value[0][0])\n",
    "\n",
    "        # Calculate SSIM\n",
    "        # ssim_loss_value, _ = ssim(ground_mri.numpy(), generator_g(image_x).numpy(), full=True)\n",
    "        # ssim_loss_values.append(ssim_loss_value)\n",
    "\n",
    "        if n % 10 == 0:\n",
    "            progress_bar.set_postfix({\n",
    "                'Elapsed Time': time.time() - start,\n",
    "                'MSE Loss': mse_loss_value.numpy(),\n",
    "                'Cosine Similarity': cosine_similarity_value[0][0],\n",
    "                # 'SSI Loss': ssim_loss_value,\n",
    "            })\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    generate_images(generator_g, ground_ct)\n",
    "\n",
    "    # Calculate and print the mean loss values for the epoch\n",
    "    mean_mse_loss = sum(mse_loss_values) / len(mse_loss_values)\n",
    "    mean_cosine_similarity = sum(cosine_similarity_values) / len(cosine_similarity_values)\n",
    "    # mean_ssim_loss = sum(ssim_loss_values) / len(ssim_loss_values)\n",
    "\n",
    "    print(f'Mean Losses for Epoch {epoch + 1}:')\n",
    "    print(f'MSE Loss: {mean_mse_loss}')\n",
    "    print(f'Mean Cosine Similarity: {mean_cosine_similarity}')\n",
    "    print(f'Mean SSIM Loss: {mean_ssim_loss}')\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))\n",
    "\n",
    "    print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n6px_r9W4a_"
   },
   "source": [
    "# Debug cell for fixing problems within the training loop or data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "ZvJVk0PYyp7D"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# MSE loss function\n",
    "def mse_loss(target, predicted):\n",
    "    return tf.reduce_mean(tf.square(target - predicted))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    n = 0\n",
    "    mse_loss_values = []  # To store MSE loss values for each iteration\n",
    "\n",
    "    progress_bar = tqdm(tf.data.Dataset.zip((train_ct, train_mri)), desc=f'Epoch {epoch + 1}', total=len(train_ct))\n",
    "\n",
    "    for image_x, image_y in progress_bar:\n",
    "        train_step(image_x, image_y)\n",
    "        mse_loss_value = mse_loss(ground_mri, generator_g(image_x))\n",
    "        mse_loss_values.append(mse_loss_value.numpy())\n",
    "        gen_g_loss_value = generator_loss(discriminator_y(generator_g(image_x)))\n",
    "        gen_f_loss_value = generator_loss(discriminator_x(generator_f(image_y)))\n",
    "\n",
    "        total_cycle_loss_value = (\n",
    "            calc_cycle_loss(image_x, generator_f(generator_g(image_x))) +\n",
    "            calc_cycle_loss(image_y, generator_g(generator_f(image_y)))\n",
    "        )\n",
    "\n",
    "        identity_loss_value = (\n",
    "            identity_loss(image_x, generator_f(image_x)) +\n",
    "            identity_loss(image_y, generator_g(image_y))\n",
    "        )\n",
    "\n",
    "        disc_x_loss_value = discriminator_loss(\n",
    "            discriminator_x(image_x),\n",
    "            discriminator_x(generator_f(image_y))\n",
    "        )\n",
    "\n",
    "        disc_y_loss_value = discriminator_loss(\n",
    "            discriminator_y(image_y),\n",
    "            discriminator_y(generator_g(image_x))\n",
    "        )\n",
    "\n",
    "        if n % 10 == 0:\n",
    "            progress_bar.set_postfix({\n",
    "                'Elapsed Time': time.time() - start,\n",
    "                'Gen G Loss': gen_g_loss_value.numpy(),\n",
    "                'Gen F Loss': gen_f_loss_value.numpy(),\n",
    "                'Cycle Loss': total_cycle_loss_value.numpy(),\n",
    "                'Identity Loss': identity_loss_value.numpy(),\n",
    "                'Discriminator X Loss': disc_x_loss_value.numpy(),\n",
    "                'Discriminator Y Loss': disc_y_loss_value.numpy(),\n",
    "                'MSE Loss': mse_loss_value.numpy(),\n",
    "            })\n",
    "\n",
    "        # n += 1\n",
    "\n",
    "        # if n % 10 == 0:\n",
    "        #     progress_bar.set_postfix({'Elapsed Time': time.time() - start, 'MSE Loss': mse_loss_value.numpy()})\n",
    "        # n += 1\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    generate_images(generator_g, ground_ct)\n",
    "\n",
    "    # Calculate and print the mean MSE loss for the epoch\n",
    "    mean_mse_loss = sum(mse_loss_values) / len(mse_loss_values)\n",
    "    print(f'Mean MSE Loss for Epoch {epoch + 1}: {mean_mse_loss}')\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))\n",
    "\n",
    "    print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "mj_bPWAfZDVe"
   },
   "outputs": [],
   "source": [
    "# Model Prediction: Generate MRI images for given CT images\n",
    "# 23 layers, 24 including input layer\n",
    "for ct_image in test_ct.take(5):\n",
    "    # Generate MRI image from CT using generator_f\n",
    "    generated_mri = generator_g(ct_image, training=False)\n",
    "    generated_ct = generator_f(generated_mri, training = False)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Original CT')\n",
    "    plt.imshow((ct_image[0] + 1) / 2.0)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('Generated MRI')\n",
    "    plt.imshow((generated_mri[0] + 1) / 2.0)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title('Generated CT')\n",
    "    plt.imshow((generated_ct[0] + 1) / 2.0)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1721031333120,
     "user": {
      "displayName": "AJITESH KUMAR SINGH",
      "userId": "07586593099214732362"
     },
     "user_tz": -330
    },
    "id": "4ZLLktuzKG9t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "12H_PKgnCaWRnWIbIsNnyOA511v2zfVVq",
     "timestamp": 1701752944013
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1322457,
     "sourceId": 2222626,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
